Adding Batch Processing to Cafe
-------------------------------

.What is Batch Processing?

Batch Processing is the execution of set of instructions called jobs with little or no user intervention. As such the input data for the jobs for would be collected in advance and provided in an accessible format such as in a file or in a database. In this tutorial, we will add support for batch processing orders. 

We will enhance the web interface to allow orders to be saved for batch processing later. Then we will add a scheduled batch process to run everyday at midnight to process all the orders collected throughout the day. The second part will showcase Spring Batch and how it can be integrated with Spring Integration.

NOTE: This example may seem a little contrived but there are many cases where batch processing would be ideal, especially in scenarios where security and/or atomicity are required.

.What is Spring Batch?

Spring Batch is a framework for batch processing developed on top of Spring. It consists of a batch job skeleton, job execution API, and state management infrastructure. In this tutorial, we will be focusing on the first two aspects on Spring Batch.

.Extending the Web Module

We want to add the option to collect orders for processing later at end of the day. We can do this by using the currently form and just adding an additional mapping in the controller:

To save orders in a readable format we save orders as JSON in a text file and the convenience factor we use GSON: http://code.google.com/p/google-gson/:

[source, xml]
----
<dependency>
	<groupId>com.google.code.gson</groupId>
	<artifactId>gson</artifactId>
	<version>2.2.2</version>
</dependency>
----


[source, java]
----
@Controller
@RequestMapping("/order")
public class OrderController {
	...
	
	private Gson converter = new Gson();
		
	@RequestMapping(method = RequestMethod.POST, value="batch")
	@ResponseStatus(HttpStatus.CREATED)
	public @ResponseBody String orderCreateBatch(Order order) throws IOException {
		order.setNumber(getID());
		for (OrderItem orderitem : order.getOrderItems()) {
			orderitem.setOrderNumber(order.getNumber());
		}
		String json = converter.toJson(order) + "\n";
		SimpleDateFormat format = new SimpleDateFormat("dd-MM-yy");
		String date = format.format(new Date());
		
		File file = new File("json-" + date + ".txt");
		 
		if(!file.exists()){
			file.createNewFile();
		}

		FileWriter fileWritter = new FileWriter(file.getName(),true);
	    BufferedWriter bufferWritter = new BufferedWriter(fileWritter);
	    bufferWritter.write(json);
	    bufferWritter.close();
		return "Successfully created Order #: " + order.getNumber() + " and added for Batch processing";
	}
}
----

In order to send the form data to this mapping, we add another submit button:

[source, xml]
----
<form:form commandName="order" id="order" onsubmit="return false;">
	...
	<tr>
		<td><input type="submit" value="Submit for Batch Processing" onclick="submitOrderBatch()" /></td>
	</tr>
	...
</form:form>

<script type="text/javascript">
	function submitOrderBatch() {					
		jq(function() {
			jq.post("order/batch", jq('#order').serialize(), function(data) {
				jq("#success").replaceWith(
						'<span id="success">' + data + '</span>');
				jq("#table").load("/cafe/status");
			});
		});
	}
</script>
----

The controller appends the order as json string to a file for that specifc day/month/year. 

.Configuring Batch Job

Spring Batch works with the concept of chunks. Chunks are a set of inputs that the batch job treats atomically, the chunk size can be as large(entire file/database) or as small (a single file) as you like. 

Each chunk is taken care of in a step. A step consists of a Reader, a (optional) Processor, and a Writer. The reader reads every item in the chunk one by one and hands of the individual item to the item processor, if one is configured. The processor can then do whatever it is required to and hands of the processed form of the input, the output, to the writer. The writer collects all the outputs belonging to a chunk and writes them once all items have been processed, thus there is atomicity at a chunk level

We can configure a Spring Job in the following manner:

[source, xml]
-----
<batch:job id="PlaceOrder">
	<batch:step id="order">
		<batch:tasklet transaction-manager="transactionManager">
			<batch:chunk reader="orderReader" processor="placeOrder" writer="syncWriter" chunk-completion-policy="wholeFileCompletionPolicy"/>
		</batch:tasklet>
	</batch:step>
</batch:job>
----

Now to configure the individual parts:

*Reader*

Since we saved the orders to be batch processed in a plain text file, we can use Spring Batch's in-built FlatFileItemReader:

[source, xml]
----
<bean id="orderReader" class="org.springframework.batch.item.file.FlatFileItemReader" scope="step">
	<property name="resource" value="#{jobParameters['input.file']}"/>
	<property name="lineMapper" ref="jsonToOrder"></property>
	<property name="strict" value="false"></property>
</bean>

<bean id="jsonToOrder" class="org.springframework.integration.samples.cafe.batch.JsonToOrder"/>

<bean class="org.springframework.batch.core.scope.StepScope" />
----

NOTE: The step scope is not present by default in Spring so we create a StepScope Bean.

Spring Batch has a feature called late binding which allows you to setup the resource and other fields like it during run time. Therefore, we will provide the input resource at runtime, as it will be different for every day. The strict property is set to false because there may not be any orders that need processing on a given day, so its not strictly necessary for the file to exist.

The line mapper is custom class that is responsible for converting each line from JSON to an Order object. Here is the code for the JsonToOrder class:

[source, java]
----
public class JsonToOrder implements LineMapper<Order>{
	
	private Gson converter = new Gson();

	public Order mapLine(String line, int lineNumber) throws Exception {
		Order order = converter.fromJson(line, Order.class);
		return order;
	}

}
----

Again here we are using Gson, but its only here for sake of convenience.

*Processor*

With the reader configured we can have a look at the processor. This is where we can start integrating Spring Integration and Spring Batch. The processor in a chunk step needs to implement ItemProcessor<I,O> and Spring Integration needs a gateway to publish orders to a channel, but there so reason why both can be done together.

We can do something similar to the following:

[source, xml]
----
<gateway id="cafe" service-interface="org.springframework.integration.samples.cafe.batch.PlaceOrder" default-request-channel="orders" default-reply-channel="deliveries"/>
----

[source, java]
----
@Scope("step")
public interface PlaceOrder extends ItemProcessor<Order, Delivery> {
	
	@Gateway
	Delivery process(Order item);
}
----

So with a very simple and straightforward configuration, we can link any complex or simple Spring Integration setup to a batch process. With this configuration for every item read by the FlatFlieItemReader it will pass it onto the ItemProcessor. The processor, being a gateway, sends the Order, as a payload of a message, to the orders channel and waits for the reply from the deliveries.

This configuration is synchronous and this inefficient because while the message is processed we could be sending more messages. Spring Batch allows you do this seamlessly, with AsyncItemProcessor/Writer, which delegates to a SyncItemProcessor/Writer (like the one we just setup).

NOTE: Check repo for asynchronous version.

*Writer*

Moving onto to the writer:

[source,xml]
----
<bean id="syncWriter" class="org.springframework.batch.item.file.FlatFileItemWriter" scope="step">
	<property name="resource" value="#{jobParameters['output.file']}"/>
	<property name="lineAggregator" ref="lineAggregator"></property>
</bean>


<bean id="lineAggregator" class="org.springframework.batch.item.file.transform.PassThroughLineAggregator"/>
----

As with the reader, we late bind the resource that the writer writes to. In order to determine how to collect and write the ouptuts, the ItemWriter needs a LineAggregator. Spring Batch comes with several common ones; we are using the PassThroughLineAggregator, which just calls the toString method of the object(in our case a delivery).

Now then only a small bit of the chunk step remains to be explained. For each <batch:chunk, we need either a commit-interval or a chunk-completion-policy attribute (but not both). The commit-interval determines a explicit chunk size (you can also use late binding to set the value using job parameters), while we implement more complex policies using chunk-completion-policy. In this example we will use an in-built policy that treats a chunk as the entire file:

[source,xml]
----
<bean id="wholeFileCompletionPolicy" class="org.springframework.batch.repeat.policy.DefaultResultCompletionPolicy"/>
----

*Scheduling*

With the job now configured, we need a way to run the job on a schedule. Before creating a scheduled function, we need to configure a JobLauncher and JobRepository:

[source, xml]
----
<bean id="jobRepository"
	class="org.springframework.batch.core.repository.support.MapJobRepositoryFactoryBean">
	<property name="transactionManager" ref="transactionManager" />
	<property name="isolationLevelForCreate" value="ISOLATION_DEFAULT"/>
</bean>

<bean id="jobLauncher"
	class="org.springframework.batch.core.launch.support.SimpleJobLauncher">
	<property name="jobRepository" ref="jobRepository" />
</bean>
----

For scheduling a job to run daily, an ideal tool would something akin to a unix style cron job. Quartz is an independent scheduling software that provides support for cron jobs and integrates seamlessly with Spring.

To add it to the project:

[source, xml]
----
<dependency>
	<groupId>org.quartz-scheduler</groupId>
	<artifactId>quartz</artifactId>
	<version>1.8.5</version>
</dependency>
----

NOTE: We are not using the latest version due to incompatibilities from API changes in Quartz

For the scheduling itself:

[source, java]
----
@Service
public class ProcessOrders {
	
	@Autowired
	private JobLauncher launcher;
	
	
	@Autowired
	private Job job;
	
	
	@Scheduled(cron="0 0 * * * ?")
	public void processDailyOrders() throws ... {
		JobParametersBuilder builder = new JobParametersBuilder();
		builder.addDate("today", new Date());
		SimpleDateFormat format = new SimpleDateFormat("dd-MM-yy");
		builder.addString("input.file", "file:json-" + format.format(new Date()) + ".txt");
		builder.addString("output.file", "file:processedorders-" + format.format(new Date()) + ".txt");
		launcher.run(job, builder.toJobParameters());
		System.out.println("Processing");
	}	
}
----

We inject both a job launcher and job into the class. We also assign the input.file and output.file as jobparameters so that they can be late bound in the ItemReader and ItemWriter.

Since we are using annotation, we need to let the application know to scan for annotations. So, using the task namespace: xmlns:task="http://www.springframework.org/schema/task", we can set up the schedule:

[source, xml]
----
<task:annotation-driven executor="executorWithPoolSizeRange"
	scheduler="taskScheduler" />

<task:executor id="executorWithPoolSizeRange" pool-size="5-25"
	queue-capacity="100" />

<task:scheduler id="taskScheduler" pool-size="1" />
----


.Summary